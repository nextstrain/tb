"""
Fetch metadata from NCBI SRA, filter and subsample, fetch fastq files, run snippy & tbprofiler

"""
# The workflow filepaths are written relative to this Snakefile's base directory
workdir: workflow.current_basedir

configfile: "defaults/config.yaml"

def get_tbprofiler_flag_paths(wildcards):
    subsample_output = checkpoints.filter_subsample.get(**wildcards).output.metadata_subsample
    samples = []
    with open(subsample_output, "r") as f:
        next(f)  # skip header
        for line in f:
            samples.append(line.strip().split("\t")[0])
    return expand("data/tbprofiler/flags/{sample}_flag.txt", sample=samples)

def get_snippy_flag_paths(wildcards):
    subsample_output = checkpoints.filter_subsample.get(**wildcards).output.metadata_subsample
    samples = []
    with open(subsample_output, "r") as f:
        next(f)  # skip header
        for line in f:
            samples.append(line.strip().split("\t")[0])
    return expand("data/snippy/flags/{sample}_flag.txt", sample=samples)

def get_alignment_samples(wildcards):
    filter_output = checkpoints.filter_qc.get(**wildcards).output.metadata_filtered
    samples = []
    with open(filter_output, "r") as f:
        next(f)  # skip header
        for line in f:
            samples.append(line.strip().split("\t")[0])
    return expand("data/snippy/{sample}", sample=samples)

rule all:
    input:
        clean_alignment="results/clean.full.aln",
        metadata="results/metadata.tsv",


rule download:
    """Downloading metadata from data.nextstrain.org"""
    output:
        metadata_raw = "data/metadata_raw.tsv"
    params:
        metadata_url = "https://data.nextstrain.org/files/workflows/tb/tb_metadata_illumina_20250211.tsv"
    shell:
        """
        curl -fsSL {params.metadata_url:q} --output {output.metadata_raw}
        """

def format_field_map(field_map: dict[str, str]) -> str:
    """
    Format dict to `"key1"="value1" "key2"="value2"...` for use in shell commands.
    """
    return " ".join([f'"{key}"="{value}"' for key, value in field_map.items()])

rule curate:
    input:
        metadata_raw = "data/metadata_raw.tsv",
        annotations=config["curate"]["annotations"],
    output:
        metadata_curate = "data/metadata_curate.tsv"
    log:
        "logs/curate.txt"
    benchmark:
        "benchmarks/curate.txt"
    conda:
        "envs/nextstrain.yaml"
    params:
        field_map=format_field_map(config["curate"]["field_map"]),
        strain_regex=config["curate"]["strain_regex"],
        strain_backup_fields=config["curate"]["strain_backup_fields"],
        date_fields=config["curate"]["date_fields"],
        expected_date_formats=config["curate"]["expected_date_formats"],
        articles=config["curate"]["titlecase"]["articles"],
        abbreviations=config["curate"]["titlecase"]["abbreviations"],
        titlecase_fields=config["curate"]["titlecase"]["fields"],
        annotations_id=config["curate"]["annotations_id"],
        id_field=config["curate"]["output_id_field"],
    shell:
        """
        augur curate passthru \
            --metadata {input.metadata_raw} \
            | augur curate rename \
                --field-map {params.field_map} \
            | augur curate normalize-strings \
            | augur curate transform-strain-name \
                --strain-regex {params.strain_regex} \
                --backup-fields {params.strain_backup_fields} \
            | augur curate format-dates \
                --date-fields {params.date_fields} \
                --expected-date-formats {params.expected_date_formats} \
                --failure-reporting silent \
            | augur curate titlecase \
                --titlecase-fields {params.titlecase_fields} \
                --articles {params.articles} \
                --abbreviations {params.abbreviations} \
            | augur curate apply-record-annotations \
                --annotations {input.annotations} \
                --id-field {params.annotations_id} \
                --output-metadata {output.metadata_curate} 2> {log}
        """

checkpoint filter_subsample:
    input:
        metadata_curate = "data/metadata_curate.tsv"
    output:
        metadata_subsample = "data/metadata_subsample.tsv"
    log:
        "logs/filter_subsample.txt",
    benchmark:
        "benchmarks/filter_subsample.txt"
    conda:
        "envs/nextstrain.yaml"
    shell:
        """
        augur filter \
            --metadata {input.metadata_curate} \
            --metadata-id-columns accession \
            --query "(mbases > 180 & (country != 'Uncalculated'))" \
	        --group-by country year \
	        --sequences-per-group 1 \
	        --min-date 2024 \
	        --output-metadata {output.metadata_subsample} 2> {log}
        """

rule run_tbprofiler:
    output:
        "data/tbprofiler/results/{sample}.results.json",
        touch("data/tbprofiler/flags/{sample}_flag.txt")
    params:
        outdir="data/fastq",
        tb_outdir="data/tbprofiler",
        threads=4,
        s3_bucket=config["s3_bucket"],
        output_path="data/tbprofiler/results/{sample}.results.json"
    log:
        "logs/tbprofiler_{sample}.txt",
    benchmark:
        "benchmarks/tbprofiler_{sample}.txt",
    conda:
        "envs/tb-profiler.yaml"
    shell:
        r'''
        set -o pipefail; \
        scripts/run_tbprofiler.sh {wildcards.sample} \
        {params.s3_bucket} \
        {params.output_path} \
        {params.outdir} \
        {params.tb_outdir} \
        {params.threads} 2>&1 | tee {log:q} \
        || echo "tbprofiler failed at sample {wildcards.sample}" | tee -a {log:q}
        '''

rule tbprofiler_collate:
    input:
        get_tbprofiler_flag_paths
    output:
        "data/tbprofiler/results/tbprofiler_all.txt"
    params:
        prefix="data/tbprofiler/results/tbprofiler_all",
        dir="data/tbprofiler/results"
    log:
        "logs/tbprofiler_collate.txt"
    benchmark:
        "benchmarks/tbprofiler_collate.txt"
    conda:
        "envs/tb-profiler.yaml"
    shell:
        """
        tb-profiler collate \
        --prefix {params.prefix} \
        --dir {params.dir} 2> {log}
        """

# tbprofiler output is included as input to run_snippy to ensure that 
# snippy doesn't start running until after run_tbprofiler has 
# completed downloading of fastq files.
rule run_snippy:
    input:
        get_tbprofiler_flag_paths,
    output:
        directory("data/snippy/{sample}"), 
        touch("data/snippy/flags/{sample}_flag.txt")
    params:
        outdir="data/fastq",
        snippy_outdir="data/snippy",
        reference="defaults/GCF_000195955.2_ASM19595v2_genomic.gbff",
        threads=4,
        s3_bucket=config["s3_bucket"],
        output_path="data/snippy/{sample}"
    log:
        "logs/snippy_{sample}.txt",
    benchmark:
        "benchmarks/snippy_{sample}.txt",
    conda:
        "envs/snippy.yaml"
    shell:
        r'''
        set -o pipefail; \
        scripts/run_snippy.sh {wildcards.sample} \
        {params.s3_bucket} \
        {params.output_path} \
        {params.outdir} \
        {params.snippy_outdir} \
        {params.reference} \
        {params.threads} 2>&1 | tee {log:q} \
        || echo "snippy failed at sample {wildcards.sample}" | tee -a {log:q}
        '''

# Calculate & collate quality control statistics from snippy alignment
# so that we can remove low quality samples that could cause 
# rule combine_align to fail by causing the core genome to have no SNPs
rule snippy_qc:
    input:
        get_snippy_flag_paths
    output:
        snippy_summary="data/snippy/snippy_summary_stats.tsv"
    log:
        "logs/snippy_qc.txt"
    benchmark:
        "benchmarks/snippy_qc.txt"
    conda:
        "envs/nextstrain.yaml"
    shell:
        """
        python scripts/summarize_snippy.py \
        --base_dir data/snippy 2> {log}
        """

# This is an inner join. Note that augur merge doesn't do inner joins.
# Note that tbprofiler_all.txt has Windows line endings, and tsv-utils
# can't handle those.
rule merge_metadata:
    input:
        metadata_subsample = "data/metadata_subsample.tsv",
        tbprofiler_output="data/tbprofiler/results/tbprofiler_all.txt",
        snippy_summary="data/snippy/snippy_summary_stats.tsv"
    output:
        metadata_stats="data/metadata_stats.tsv",
    log:
        "logs/merge_metadata.txt",
    benchmark:
        "benchmarks/merge_metadata.txt"
    conda:
        "envs/csvtk.yaml"
    shell:
        """
        csvtk join -t -f 1 {input.metadata_subsample} \
        {input.snippy_summary} \
        {input.tbprofiler_output} \
        > {output.metadata_stats} 2> {log}
        """

# Filter out poor-quality samples and samples that are M. canetti.
checkpoint filter_qc:
    input:
        metadata_stats="data/metadata_stats.tsv",
    output:
        metadata_filtered="data/metadata_filtered.tsv",
    log:
        "logs/filter_qc.txt"
    benchmark:
        "benchmarks/filter_qc.txt"
    conda:
        "envs/tsv-utils.yaml"
    shell:
        """
        tsv-filter --header --gt  pct_reads_mapped:80 data/metadata_stats.tsv \
        | tsv-filter --header --gt  target_median_depth:30 \
        | tsv-filter --header --gt  ALIGNED:3529226 \
        | tsv-filter --header --str-eq main_lineage:M.canetti  --invert \
        > {output.metadata_filtered} 2> {log}
        """

# Note that we had to manually change the chromosome name in the reference 
# genome from "NC_000962.3" to "NC_000962" because that is the chromosome 
# name in the gbff file that snippy used for alignment.
rule combine_align:
    input:
        sample=get_alignment_samples,
        mask_file="defaults/AF14_RLC_Regions.Plus.LowPmapK50E4.H37Rv.bed"
    output:
        alignment="data/snippy/core.full.aln",
        snippy_summary="data/snippy/core.txt"
    params:
        ref="defaults/GCF_000195955.2_ASM19595v2_genomic.fna",
        prefix="data/snippy/core"
    log:
        "logs/snippy_combine_align.txt"
    benchmark:
        "benchmarks/snippy_combine_align.txt"
    conda:
        "envs/snippy.yaml"
    shell:
        """
        snippy-core --ref {params.ref} \
        {input.sample} \
        --mask {input.mask_file} \
        --prefix {params.prefix} 2> {log}
        """

rule clean_align:
    input:
        alignment="data/snippy/core.full.aln"
    output:
        clean_alignment="results/clean.full.aln",
    log:
        "logs/snippy_clean_align.txt"
    benchmark:
        "benchmarks/snippy_clean_align.txt"
    conda:
        "envs/snippy.yaml"
    shell:
        """
        snippy-clean_full_aln \
        {input.alignment} \
        > {output.clean_alignment} 2> {log}
        """
        """

# Replace the snippy qc metrics with metrics that take into account masking.
# data/snippy/core.txt is an output from rule combine_align
rule replace_snippyqc:
    input:
        metadata_filtered = "data/metadata_filtered.tsv",
        snippy_summary="data/snippy/core.txt"
    output:
        metadata="results/metadata.tsv",
    log:
        "logs/replace_snippyqc.txt"
    benchmark:
        "benchmarks/replace_snippyqc.txt"
    conda:
        "envs/nextstrain.yaml"
    shell:
        """
        augur merge \
        --metadata filtered={input.metadata_filtered} \
        snippy={input.snippy_summary} \
        --metadata-id-columns 'accession' 'ID' \
        --output-metadata {output.metadata} 2> {log}
        """
